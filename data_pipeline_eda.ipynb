{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subscriber Cancellations Data Pipeline\n",
    "\n",
    "For this project, you’ll build a data engineering pipeline to regularly transform a messy database into a clean source of truth for an analytics team\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "\n",
    "Download the starter kit and set up your working directory to explore the data! When you’re ready, connect to the starter database by loading `dev/cademycode.db` in a Jupyter notebook.\n",
    "\n",
    "**********\n",
    "+ Download the starter kit\n",
    "+ Unzip the starter kit\n",
    "+ You should get a folder containing a folder `/dev`\n",
    "+ Create a Jupyter notebook in the `/dev` folder for your initial exploration\n",
    "+ Use the sqlite3 Python package or SQLAlchemy to establish a database connection\n",
    "+ Use the database `/cademycode.db` for your development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database: ['cademycode_courses', 'cademycode_student_jobs', 'cademycode_students']\n",
      "Datos de la tabla cademycode_courses:\n",
      "   career_path_id      career_path_name  hours_to_complete\n",
      "0               1        data scientist                 20\n",
      "1               2         data engineer                 20\n",
      "2               3          data analyst                 12\n",
      "3               4  software engineering                 25\n",
      "4               5      backend engineer                 18\n",
      "Datos de la tabla cademycode_student_jobs:\n",
      "   job_id        job_category  avg_salary\n",
      "0       1           analytics       86000\n",
      "1       2            engineer      101000\n",
      "2       3  software developer      110000\n",
      "3       4            creative       66000\n",
      "4       5  financial services      135000\n",
      "Datos de la tabla cademycode_students:\n",
      "   uuid             name         dob sex  \\\n",
      "0     1  Annabelle Avery  1943-07-03   F   \n",
      "1     2      Micah Rubio  1991-02-07   M   \n",
      "2     3       Hosea Dale  1989-12-07   M   \n",
      "3     4     Mariann Kirk  1988-07-31   F   \n",
      "4     5  Lucio Alexander  1963-08-31   M   \n",
      "\n",
      "                                        contact_info job_id num_course_taken  \\\n",
      "0  {\"mailing_address\": \"303 N Timber Key, Irondal...    7.0              6.0   \n",
      "1  {\"mailing_address\": \"767 Crescent Fair, Shoals...    7.0              5.0   \n",
      "2  {\"mailing_address\": \"P.O. Box 41269, St. Bonav...    7.0              8.0   \n",
      "3  {\"mailing_address\": \"517 SE Wintergreen Isle, ...    6.0              7.0   \n",
      "4  {\"mailing_address\": \"18 Cinder Cliff, Doyles b...    7.0             14.0   \n",
      "\n",
      "  current_career_path_id time_spent_hrs  \n",
      "0                    1.0           4.99  \n",
      "1                    8.0            4.4  \n",
      "2                    8.0           6.74  \n",
      "3                    9.0          12.31  \n",
      "4                    3.0           5.64  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "# Establish the database connection\n",
    "engine = create_engine('sqlite:///cademycode.db')\n",
    "\n",
    "# Create an inspector to retrieve table names\n",
    "inspector = inspect(engine)\n",
    "tables = inspector.get_table_names()\n",
    "print(\"Tables in the database:\", tables)\n",
    "\n",
    "# # Read data from each table and display it\n",
    "# for table in tables:\n",
    "#     query = f'SELECT * FROM {table}'\n",
    "#     df = pd.read_sql_query(query, con=engine)\n",
    "#     print(f\"Data from {table} table:\")\n",
    "#     print(df.head())  # Display the first 5 rows of each table\n",
    "\n",
    "# Read data from each table and display it -> Updated \n",
    "dataframes = {}\n",
    "for table in tables:\n",
    "    query = f'SELECT * FROM {table}'\n",
    "    df = pd.read_sql_query(query, con=engine)\n",
    "    dataframes[table] = df\n",
    "    print(f\"Datos de la tabla {table}:\")\n",
    "    print(df.head())  # Display the first 5 rows of each table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect and Clean the data\n",
    "\n",
    "Import the tables in cademycode.db as dataframes. Inspect the tables for missing or invalid data and perform any data cleaning operations you think are necessary.\n",
    "\n",
    "************\n",
    "Here are some tips to get started (but remember, there are many different routes to take):\n",
    "\n",
    "+ Use a `SELECT` query on `sqlite_master` to determine the names of the tables\n",
    "+ Use `pandas.read_sql_query` to read each table in as a DataFrame\n",
    "+ Get familiar with the data by using the `.head()` function to explore the first handful of rows in the database.\n",
    "+ Look for null values or invalide datatypes by using\n",
    "+ `.info()` to display a summary table of each column\n",
    "+ `.describe()` to calculate summary statistics for all numerical columns\n",
    "+ `.value_counts()` to display each column’s distinct values.\n",
    "+  Check out our data wrangling and tidying refresher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database: ['cademycode_courses', 'cademycode_student_jobs', 'cademycode_students']\n"
     ]
    }
   ],
   "source": [
    "# Use a SELECT query on sqlite_master to determine the names of the tables\n",
    "query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "tables = pd.read_sql_query(query, con=engine)\n",
    "table_names = tables['name'].tolist()\n",
    "print(\"Tables in the database:\", table_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data from cademycode_courses table:\n",
      "   career_path_id      career_path_name  hours_to_complete\n",
      "0               1        data scientist                 20\n",
      "1               2         data engineer                 20\n",
      "2               3          data analyst                 12\n",
      "3               4  software engineering                 25\n",
      "4               5      backend engineer                 18\n",
      "\n",
      "Data from cademycode_student_jobs table:\n",
      "   job_id        job_category  avg_salary\n",
      "0       1           analytics       86000\n",
      "1       2            engineer      101000\n",
      "2       3  software developer      110000\n",
      "3       4            creative       66000\n",
      "4       5  financial services      135000\n",
      "\n",
      "Data from cademycode_students table:\n",
      "   uuid             name         dob sex  \\\n",
      "0     1  Annabelle Avery  1943-07-03   F   \n",
      "1     2      Micah Rubio  1991-02-07   M   \n",
      "2     3       Hosea Dale  1989-12-07   M   \n",
      "3     4     Mariann Kirk  1988-07-31   F   \n",
      "4     5  Lucio Alexander  1963-08-31   M   \n",
      "\n",
      "                                        contact_info job_id num_course_taken  \\\n",
      "0  {\"mailing_address\": \"303 N Timber Key, Irondal...    7.0              6.0   \n",
      "1  {\"mailing_address\": \"767 Crescent Fair, Shoals...    7.0              5.0   \n",
      "2  {\"mailing_address\": \"P.O. Box 41269, St. Bonav...    7.0              8.0   \n",
      "3  {\"mailing_address\": \"517 SE Wintergreen Isle, ...    6.0              7.0   \n",
      "4  {\"mailing_address\": \"18 Cinder Cliff, Doyles b...    7.0             14.0   \n",
      "\n",
      "  current_career_path_id time_spent_hrs  \n",
      "0                    1.0           4.99  \n",
      "1                    8.0            4.4  \n",
      "2                    8.0           6.74  \n",
      "3                    9.0          12.31  \n",
      "4                    3.0           5.64  \n"
     ]
    }
   ],
   "source": [
    "# Read data from each table and display it\n",
    "dataframes = {}\n",
    "for table in table_names:\n",
    "    df = pd.read_sql_query(f'SELECT * FROM {table}', con=engine)\n",
    "    dataframes[table] = df\n",
    "    print(f\"\\nData from {table} table:\")\n",
    "    print(df.head())  # Display the first 5 rows of each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting cademycode_courses table:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   career_path_id     10 non-null     int64 \n",
      " 1   career_path_name   10 non-null     object\n",
      " 2   hours_to_complete  10 non-null     int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 368.0+ bytes\n",
      "None\n",
      "       career_path_id  hours_to_complete\n",
      "count        10.00000          10.000000\n",
      "mean          5.50000          21.900000\n",
      "std           3.02765           6.707376\n",
      "min           1.00000          12.000000\n",
      "25%           3.25000          18.500000\n",
      "50%           5.50000          20.000000\n",
      "75%           7.75000          26.500000\n",
      "max          10.00000          35.000000\n",
      "\n",
      "Value counts for career_path_id column:\n",
      "career_path_id\n",
      "1     1\n",
      "2     1\n",
      "3     1\n",
      "4     1\n",
      "5     1\n",
      "6     1\n",
      "7     1\n",
      "8     1\n",
      "9     1\n",
      "10    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for career_path_name column:\n",
      "career_path_name\n",
      "data scientist               1\n",
      "data engineer                1\n",
      "data analyst                 1\n",
      "software engineering         1\n",
      "backend engineer             1\n",
      "frontend engineer            1\n",
      "iOS developer                1\n",
      "android developer            1\n",
      "machine learning engineer    1\n",
      "ux/ui designer               1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for hours_to_complete column:\n",
      "hours_to_complete\n",
      "20    3\n",
      "27    2\n",
      "12    1\n",
      "25    1\n",
      "18    1\n",
      "35    1\n",
      "15    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values in cademycode_courses table:\n",
      "career_path_id       0\n",
      "career_path_name     0\n",
      "hours_to_complete    0\n",
      "dtype: int64\n",
      "\n",
      "Cleaned data from cademycode_courses table:\n",
      "   career_path_id      career_path_name  hours_to_complete\n",
      "0               1        data scientist                 20\n",
      "1               2         data engineer                 20\n",
      "2               3          data analyst                 12\n",
      "3               4  software engineering                 25\n",
      "4               5      backend engineer                 18\n",
      "\n",
      "Inspecting cademycode_student_jobs table:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13 entries, 0 to 12\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   job_id        13 non-null     int64 \n",
      " 1   job_category  13 non-null     object\n",
      " 2   avg_salary    13 non-null     int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 440.0+ bytes\n",
      "None\n",
      "          job_id     avg_salary\n",
      "count  13.000000      13.000000\n",
      "mean    4.384615   89230.769231\n",
      "std     2.662657   34727.879881\n",
      "min     0.000000   10000.000000\n",
      "25%     3.000000   66000.000000\n",
      "50%     4.000000   86000.000000\n",
      "75%     6.000000  110000.000000\n",
      "max     9.000000  135000.000000\n",
      "\n",
      "Value counts for job_id column:\n",
      "job_id\n",
      "3    2\n",
      "4    2\n",
      "5    2\n",
      "1    1\n",
      "2    1\n",
      "6    1\n",
      "7    1\n",
      "8    1\n",
      "9    1\n",
      "0    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for job_category column:\n",
      "job_category\n",
      "software developer    2\n",
      "creative              2\n",
      "financial services    2\n",
      "analytics             1\n",
      "engineer              1\n",
      "education             1\n",
      "HR                    1\n",
      "student               1\n",
      "healthcare            1\n",
      "other                 1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for avg_salary column:\n",
      "avg_salary\n",
      "110000    2\n",
      "66000     2\n",
      "135000    2\n",
      "80000     2\n",
      "86000     1\n",
      "101000    1\n",
      "61000     1\n",
      "10000     1\n",
      "120000    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values in cademycode_student_jobs table:\n",
      "job_id          0\n",
      "job_category    0\n",
      "avg_salary      0\n",
      "dtype: int64\n",
      "\n",
      "Cleaned data from cademycode_student_jobs table:\n",
      "   job_id        job_category  avg_salary\n",
      "0       1           analytics       86000\n",
      "1       2            engineer      101000\n",
      "2       3  software developer      110000\n",
      "3       4            creative       66000\n",
      "4       5  financial services      135000\n",
      "\n",
      "Inspecting cademycode_students table:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 9 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   uuid                    5000 non-null   int64 \n",
      " 1   name                    5000 non-null   object\n",
      " 2   dob                     5000 non-null   object\n",
      " 3   sex                     5000 non-null   object\n",
      " 4   contact_info            5000 non-null   object\n",
      " 5   job_id                  5000 non-null   object\n",
      " 6   num_course_taken        5000 non-null   object\n",
      " 7   current_career_path_id  5000 non-null   object\n",
      " 8   time_spent_hrs          5000 non-null   object\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 351.7+ KB\n",
      "None\n",
      "              uuid\n",
      "count  5000.000000\n",
      "mean   2500.500000\n",
      "std    1443.520003\n",
      "min       1.000000\n",
      "25%    1250.750000\n",
      "50%    2500.500000\n",
      "75%    3750.250000\n",
      "max    5000.000000\n",
      "\n",
      "Value counts for uuid column:\n",
      "uuid\n",
      "1       1\n",
      "3331    1\n",
      "3338    1\n",
      "3337    1\n",
      "3336    1\n",
      "       ..\n",
      "1667    1\n",
      "1666    1\n",
      "1665    1\n",
      "1664    1\n",
      "5000    1\n",
      "Name: count, Length: 5000, dtype: int64\n",
      "\n",
      "Value counts for name column:\n",
      "name\n",
      "Melvin Felt              2\n",
      "Robbie Davies            2\n",
      "Annabelle Avery          1\n",
      "Kelley Munnickhuijsen    1\n",
      "Rachel de Kruijff        1\n",
      "                        ..\n",
      "Cleopatra Singleton      1\n",
      "Linwood Patrick          1\n",
      "Marcia Beeldhouwer       1\n",
      "Arnoldo Rodgers          1\n",
      "Elton Otway              1\n",
      "Name: count, Length: 4998, dtype: int64\n",
      "\n",
      "Value counts for dob column:\n",
      "dob\n",
      "1993-08-03    4\n",
      "1955-05-27    4\n",
      "1953-12-05    3\n",
      "1995-06-13    3\n",
      "1956-05-22    3\n",
      "             ..\n",
      "1960-09-21    1\n",
      "1992-05-08    1\n",
      "1979-03-22    1\n",
      "1966-12-30    1\n",
      "1994-12-23    1\n",
      "Name: count, Length: 4492, dtype: int64\n",
      "\n",
      "Value counts for sex column:\n",
      "sex\n",
      "M    1995\n",
      "F    1990\n",
      "N    1015\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for contact_info column:\n",
      "contact_info\n",
      "{\"mailing_address\": \"303 N Timber Key, Irondale, Wisconsin, 84736\", \"email\": \"annabelle_avery9376@woohoo.com\"}      1\n",
      "{\"mailing_address\": \"768 Silent Skyway, Impact, Idaho, 90270\", \"email\": \"orval_devos4502@coldmail.com\"}             1\n",
      "{\"mailing_address\": \"409 SW First Pike, Leary, North Carolina, 43428\", \"email\": \"rachel886@woohoo.com\"}             1\n",
      "{\"mailing_address\": \"554 Broad Rose, Flagler Beach, Florida, 65799\", \"email\": \"allan8306@inlook.com\"}               1\n",
      "{\"mailing_address\": \"872 Wagon Land, Guthrie Center, Colorado, 86498\", \"email\": \"isidro3025@woohoo.com\"}            1\n",
      "                                                                                                                   ..\n",
      "{\"mailing_address\": \"470 Essex Curve, Copan, Mississippi, 86309\", \"email\": \"cleopatra_singleton7791@inlook.com\"}    1\n",
      "{\"mailing_address\": \"162 Iron Chase, Campbell Station, Oklahoma, 78795\", \"email\": \"patrick6416@inlook.com\"}         1\n",
      "{\"mailing_address\": \"889 Mountain Alley, Quioque, Florida, 22930\", \"email\": \"marcia9217@coldmail.com\"}              1\n",
      "{\"mailing_address\": \"P.O. Box 69948, Hastings borough, Iowa, 13359\", \"email\": \"rodgers8447@hmail.com\"}              1\n",
      "{\"mailing_address\": \"406 Zephyr Port, Oskaloosa, Alabama, 74534\", \"email\": \"elton_otway5644@inlook.com\"}            1\n",
      "Name: count, Length: 5000, dtype: int64\n",
      "\n",
      "Value counts for job_id column:\n",
      "job_id\n",
      "2.0    706\n",
      "1.0    693\n",
      "7.0    681\n",
      "3.0    675\n",
      "4.0    672\n",
      "5.0    662\n",
      "6.0    658\n",
      "8.0    253\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for num_course_taken column:\n",
      "num_course_taken\n",
      "5.0     354\n",
      "12.0    347\n",
      "2.0     330\n",
      "10.0    323\n",
      "15.0    323\n",
      "0.0     318\n",
      "7.0     316\n",
      "13.0    316\n",
      "8.0     310\n",
      "11.0    303\n",
      "4.0     299\n",
      "6.0     296\n",
      "3.0     296\n",
      "14.0    292\n",
      "1.0     291\n",
      "9.0     286\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for current_career_path_id column:\n",
      "current_career_path_id\n",
      "5.0     525\n",
      "3.0     513\n",
      "10.0    509\n",
      "7.0     505\n",
      "6.0     504\n",
      "1.0     501\n",
      "2.0     498\n",
      "9.0     485\n",
      "4.0     483\n",
      "8.0     477\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for time_spent_hrs column:\n",
      "time_spent_hrs\n",
      "17.47    9\n",
      "4.71     9\n",
      "2.91     8\n",
      "5.93     8\n",
      "13.19    8\n",
      "        ..\n",
      "27.68    1\n",
      "17.8     1\n",
      "12.41    1\n",
      "21.77    1\n",
      "23.54    1\n",
      "Name: count, Length: 2192, dtype: int64\n",
      "\n",
      "Missing values in cademycode_students table:\n",
      "uuid                      0\n",
      "name                      0\n",
      "dob                       0\n",
      "sex                       0\n",
      "contact_info              0\n",
      "job_id                    0\n",
      "num_course_taken          0\n",
      "current_career_path_id    0\n",
      "time_spent_hrs            0\n",
      "dtype: int64\n",
      "\n",
      "Cleaned data from cademycode_students table:\n",
      "   uuid             name         dob sex  \\\n",
      "0     1  Annabelle Avery  1943-07-03   F   \n",
      "1     2      Micah Rubio  1991-02-07   M   \n",
      "2     3       Hosea Dale  1989-12-07   M   \n",
      "3     4     Mariann Kirk  1988-07-31   F   \n",
      "4     5  Lucio Alexander  1963-08-31   M   \n",
      "\n",
      "                                        contact_info job_id num_course_taken  \\\n",
      "0  {\"mailing_address\": \"303 N Timber Key, Irondal...    7.0              6.0   \n",
      "1  {\"mailing_address\": \"767 Crescent Fair, Shoals...    7.0              5.0   \n",
      "2  {\"mailing_address\": \"P.O. Box 41269, St. Bonav...    7.0              8.0   \n",
      "3  {\"mailing_address\": \"517 SE Wintergreen Isle, ...    6.0              7.0   \n",
      "4  {\"mailing_address\": \"18 Cinder Cliff, Doyles b...    7.0             14.0   \n",
      "\n",
      "  current_career_path_id time_spent_hrs  \n",
      "0                    1.0           4.99  \n",
      "1                    8.0            4.4  \n",
      "2                    8.0           6.74  \n",
      "3                    9.0          12.31  \n",
      "4                    3.0           5.64  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri_7a484pu\\AppData\\Local\\Temp\\ipykernel_41260\\3233913241.py:17: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_cleaned = df.fillna(method='ffill').fillna(method='bfill')  # Forward fill then backward fill as an example\n",
      "C:\\Users\\gabri_7a484pu\\AppData\\Local\\Temp\\ipykernel_41260\\3233913241.py:17: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_cleaned = df.fillna(method='ffill').fillna(method='bfill')  # Forward fill then backward fill as an example\n",
      "C:\\Users\\gabri_7a484pu\\AppData\\Local\\Temp\\ipykernel_41260\\3233913241.py:17: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_cleaned = df.fillna(method='ffill').fillna(method='bfill')  # Forward fill then backward fill as an example\n"
     ]
    }
   ],
   "source": [
    "# Inspect and clean the data\n",
    "for table, df in dataframes.items():\n",
    "    print(f\"\\nInspecting {table} table:\")\n",
    "    print(df.info())  # Display summary of each column\n",
    "    print(df.describe())  # Calculate summary statistics for numerical columns\n",
    "    for column in df.columns:\n",
    "        print(f\"\\nValue counts for {column} column:\")\n",
    "        print(df[column].value_counts())  # Display distinct values of each column\n",
    "\n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(f\"\\nMissing values in {table} table:\")\n",
    "    print(missing_values)\n",
    "\n",
    "    # Perform data cleaning operations if necessary\n",
    "    # Example: Fill missing values with appropriate values or drop rows/columns with missing values\n",
    "    df_cleaned = df.fillna(method='ffill').fillna(method='bfill')  # Forward fill then backward fill as an example\n",
    "    dataframes[table] = df_cleaned\n",
    "\n",
    "    print(f\"\\nCleaned data from {table} table:\")\n",
    "    print(df_cleaned.head())  # Display the first 5 rows of cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data back to the database or to new files if needed\n",
    "for table, df_cleaned in dataframes.items():\n",
    "    df_cleaned.to_sql(table, con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Output CSV\n",
    "Use the cleaned tables to produce an analytics-ready SQLite database and flat CSV file. The final CSV should contain all the data the analysts might need in a single table.\n",
    "\n",
    "************\n",
    "\n",
    "+ Think about what fields you might want to have as an analyst – are there any you can create as part of this process?\n",
    "+ It is easier to update a database than update a CSV file, so create a clean SQLite database first, and then generate the CSV from that database.\n",
    "+ You might want to create the CSV by joining the original three tables into one. One way is to use the pandas `.merge()` function. Another would be to use SQL `join` within the SQLite database.\n",
    "+ Make sure to validate the final table. Improper joins could result in losing rows due to unpaired keys or duplication. You can check for both by calculating the length of your dataframe before and after merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cleaned data from the database\n",
    "df_courses = pd.read_sql_query('SELECT * FROM cademycode_courses', con=engine)\n",
    "df_jobs = pd.read_sql_query('SELECT * FROM cademycode_student_jobs', con=engine)\n",
    "df_students = pd.read_sql_query('SELECT * FROM cademycode_students', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert job_id columns to integers, handling any non-integer values\n",
    "df_students['job_id'] = pd.to_numeric(df_students['job_id'], errors='coerce').fillna(0).astype('int64')\n",
    "df_jobs['job_id'] = pd.to_numeric(df_jobs['job_id'], errors='coerce').fillna(0).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the tables into a single DataFrame\n",
    "# Merge students with jobs\n",
    "df_merged = pd.merge(df_students, df_jobs, how='left', left_on='job_id', right_on='job_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the current_career_path_id and career_path_id columns have the same data type\n",
    "df_merged['current_career_path_id'] = pd.to_numeric(df_merged['current_career_path_id'], errors='coerce').fillna(0).astype('int64')\n",
    "df_courses['career_path_id'] = pd.to_numeric(df_courses['career_path_id'], errors='coerce').fillna(0).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the result with courses\n",
    "df_final = pd.merge(df_merged, df_courses, how='left', left_on='current_career_path_id', right_on='career_path_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of DataFrame before merge: 5000\n",
      "Length of DataFrame after merge: 7009\n"
     ]
    }
   ],
   "source": [
    "# Validate the final table\n",
    "print(\"Length of DataFrame before merge:\", len(df_students))\n",
    "print(\"Length of DataFrame after merge:\", len(df_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in the final DataFrame:\n",
      "uuid                      0\n",
      "name                      0\n",
      "dob                       0\n",
      "sex                       0\n",
      "contact_info              0\n",
      "job_id                    0\n",
      "num_course_taken          0\n",
      "current_career_path_id    0\n",
      "time_spent_hrs            0\n",
      "job_category              0\n",
      "avg_salary                0\n",
      "career_path_id            0\n",
      "career_path_name          0\n",
      "hours_to_complete         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for any missing values after the merge\n",
    "print(\"Missing values in the final DataFrame:\")\n",
    "print(df_final.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7009"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the final DataFrame to a new SQLite database\n",
    "engine_clean = create_engine('sqlite:///cademycode_clean.db')\n",
    "df_final.to_sql('cademycode_final', con=engine_clean, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final CSV and SQLite database created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the final DataFrame to a CSV file\n",
    "df_final.to_csv('cademycode_final.csv', index=False)\n",
    "print(\"Final CSV and SQLite database created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop Unit Tests and Logs\n",
    "Turn the Jupyter Notebook into a Python script that can be run with minimal human intervention.\n",
    "\n",
    "The script should:\n",
    "\n",
    "+ check for updates to the database and\n",
    "+ use unit tests to protect the update process.\n",
    "\n",
    "Any updates made to the final database should be written to a changelog, and any errors from the unit tests should be written to an error log.\n",
    "\n",
    "************\n",
    "\n",
    "Some ideas for unit tests include:\n",
    "\n",
    "+ checking that the updated database has the same schema as the original\n",
    "+ checking if the tables will join properly\n",
    "+ checking if there is any new data\n",
    "\n",
    "Your changelog should include details like:\n",
    "\n",
    "+ a version number\n",
    "+ information about the update such as new row and missing data counts\n",
    "\n",
    "\n",
    "Some helpful resources include\n",
    "\n",
    "+ Logging in Python\n",
    "+ Errors in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar los scripts y asegurarte de que todo funcione correctamente, sigue estos pasos:\n",
    "\n",
    "### 1. Configurar el Entorno\n",
    "\n",
    "Asegúrate de tener un entorno de Python configurado con las bibliotecas necesarias instaladas. Puedes usar `pip` para instalar las dependencias:\n",
    "\n",
    "```bash\n",
    "pip install pandas sqlalchemy\n",
    "```\n",
    "\n",
    "### 2. Ejecutar el Script Principal\n",
    "\n",
    "Ejecuta el script principal (`data_pipeline.py`) para procesar los datos y generar la base de datos y el archivo CSV:\n",
    "\n",
    "```bash\n",
    "python data_pipeline.py\n",
    "```\n",
    "\n",
    "### 3. Verificar los Resultados\n",
    "\n",
    "Después de ejecutar el script principal, verifica que se hayan creado los siguientes archivos en el directorio `dev`:\n",
    "- `cademycode_clean.db`: La base de datos SQLite limpia.\n",
    "- `cademycode_final.csv`: El archivo CSV con los datos finales.\n",
    "\n",
    "### 4. Ejecutar las Pruebas Unitarias\n",
    "\n",
    "Ejecuta el archivo de pruebas (`test_data_pipeline.py`) para asegurarte de que las pruebas unitarias pasen correctamente:\n",
    "\n",
    "```bash\n",
    "python -m unittest test_data_pipeline.py\n",
    "```\n",
    "\n",
    "### 5. Revisar los Registros\n",
    "\n",
    "Revisa el archivo de registro `data_pipeline.log` para ver los detalles de la ejecución del script, incluyendo cualquier error que haya ocurrido.\n",
    "\n",
    "### Changelog\n",
    "\n",
    "El changelog es un archivo de texto (`changelog.txt`) donde puedes registrar los detalles de cada actualización de la base de datos. Cada vez que ejecutes el script principal, puedes agregar una entrada al changelog con información como:\n",
    "\n",
    "- **Número de versión**: Un número de versión para la actualización (por ejemplo, `1.0.0`).\n",
    "- **Detalles de la actualización**: Información sobre la actualización, como el número de nuevas filas agregadas y la cantidad de datos faltantes.\n",
    "\n",
    "Aquí tienes un ejemplo de cómo podrías actualizar el changelog:\n",
    "\n",
    "```plaintext\n",
    "Version 1.0.0\n",
    "- New rows added: 100\n",
    "- Missing data counts: 0\n",
    "```\n",
    "\n",
    "Puedes agregar esta entrada manualmente después de cada ejecución del script principal, o puedes modificar el script para que lo haga automáticamente.\n",
    "\n",
    "### Resumen de los Pasos:\n",
    "\n",
    "1. **Configurar el entorno**: Instalar las dependencias necesarias.\n",
    "2. **Ejecutar el script principal**: Procesar los datos y generar la base de datos y el archivo CSV.\n",
    "3. **Verificar los resultados**: Asegurarte de que los archivos se hayan creado correctamente.\n",
    "4. **Ejecutar las pruebas unitarias**: Verificar que las pruebas pasen correctamente.\n",
    "5. **Revisar los registros**: Verificar el archivo de registro para detalles de la ejecución.\n",
    "6. **Actualizar el changelog**: Registrar los detalles de la actualización en el changelog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tablas en la base de datos: [('cademycode_courses',), ('cademycode_student_jobs',), ('cademycode_students',)]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect('cademycode.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Check existing tables\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "print(\"Tables in the database:\", tables)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DUMB DB\n",
    "+ cademycode_dump.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Establish the connection to the PostgreSQL database\n",
    "# engine = create_engine('postgresql://postgres:postgres@localhost/subscriber-pipeline')\n",
    "engine = create_engine('sqlite:///cademycode.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>career_path_id</th>\n",
       "      <th>career_path_name</th>\n",
       "      <th>hours_to_complete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>data engineer</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>data analyst</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>software engineering</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>backend engineer</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>frontend engineer</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>iOS developer</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>android developer</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>machine learning engineer</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>ux/ui designer</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   career_path_id           career_path_name  hours_to_complete\n",
       "0               1             data scientist                 20\n",
       "1               2              data engineer                 20\n",
       "2               3               data analyst                 12\n",
       "3               4       software engineering                 25\n",
       "4               5           backend engineer                 18\n",
       "5               6          frontend engineer                 20\n",
       "6               7              iOS developer                 27\n",
       "7               8          android developer                 27\n",
       "8               9  machine learning engineer                 35\n",
       "9              10             ux/ui designer                 15"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_courses = pd.read_sql_query('SELECT * FROM \"cademycode_courses\"', con=engine)\n",
    "df_courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database: [('cademycode_final_local',)]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect('cademycode_clean_local.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Check existing tables\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "print(\"Tables in the database:\", tables)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Bash Script\n",
    "Create a bash script to handle running the Python script and moving updated files from your working directory in `/dev` to a production directory. Your bash script should use the logs from the last task to determine if an update occurred.\n",
    "\n",
    "****\n",
    "\n",
    "+ Here’s our bash scripting course, if you find yourself stuck!\n",
    "+ You can execute python files within bash scripts by calling `python path/to/source/file.py`.\n",
    "+ You can either chose to move the file over to the production folder using `mv /path/to/source /path/to/destination` or copy the files to the production folder by using `cp /path/to/source /path/to/destination`\n",
    "+ Use version numbers in your changelog to check for updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's adjust the Bash script to create the `prod` directory if it doesn't exist and then move the generated files to that directory.**\n",
    "\n",
    "\n",
    "### Adjusted Bash Script\n",
    "\n",
    "`run_pipeline.sh`\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Directories\n",
    "DEV_DIR=\"./dev\"\n",
    "PROD_DIR=\"./prod\"\n",
    "LOG_FILE=\"$DEV_DIR/data_pipeline.log\"\n",
    "CHANGELOG_FILE=\"$DEV_DIR/changelog.txt\"\n",
    "\n",
    "# Activate the virtual environment\n",
    "source \"C:/Users/gabri_7a484pu/Escritorio/code/DataEngineer_Codecademy/.venv/Scripts/activate\"\n",
    "\n",
    "# Create the production directory if it doesn't exist\n",
    "if [ ! -d \"$PROD_DIR\" ]; then\n",
    "    mkdir \"$PROD_DIR\"\n",
    "    echo \"Production directory created: $PROD_DIR\"\n",
    "else\n",
    "    echo \"Production directory already exists: $PROD_DIR\"\n",
    "fi\n",
    "\n",
    "# Run the main Python script\n",
    "echo \"Running the main Python script.\"\n",
    "python ./data_pipeline.py\n",
    "\n",
    "# Check that the files have been created\n",
    "if [ -f \"$DEV_DIR/cademycode_clean_local.db\" ] && [ -f \"$DEV_DIR/cademycode_final_local.csv\" ]; then\n",
    "    echo \"Files generated successfully.\"\n",
    "\n",
    "    # Run the unit tests\n",
    "    echo \"Running the unit tests.\"\n",
    "    python -m unittest ./test_data_pipeline.py\n",
    "\n",
    "    # Check the logs for errors\n",
    "    echo \"Checking the logs for errors.\"\n",
    "    if grep -q \"ERROR\" \"$LOG_FILE\"; then\n",
    "        echo \"Errors found in the log. Check $LOG_FILE for more details.\"\n",
    "    else\n",
    "        echo \"No errors found in the log.\"\n",
    "\n",
    "        # Check if there was an update in the changelog\n",
    "        echo \"Checking if there was an update in the changelog.\"\n",
    "        if grep -q \"Version\" \"$CHANGELOG_FILE\"; then\n",
    "            echo \"Update detected. Moving files to production...\"\n",
    "\n",
    "            # Check that the files exist before moving them\n",
    "            if [ -f \"$DEV_DIR/cademycode_clean_local.db\" ]; then\n",
    "                echo \"Moving cademycode_clean_local.db to production.\"\n",
    "                mv $DEV_DIR/cademycode_clean_local.db $PROD_DIR/\n",
    "                echo \"File cademycode_clean_local.db moved to production.\"\n",
    "            else\n",
    "                echo \"File cademycode_clean_local.db not found.\"\n",
    "            fi\n",
    "\n",
    "            if [ -f \"$DEV_DIR/cademycode_final_local.csv\" ]; then\n",
    "                echo \"Moving cademycode_final_local.csv to production.\"\n",
    "                mv $DEV_DIR/cademycode_final_local.csv $PROD_DIR/\n",
    "                echo \"File cademycode_final_local.csv moved to production.\"\n",
    "            else\n",
    "                echo \"File cademycode_final_local.csv not found.\"\n",
    "            fi\n",
    "\n",
    "            echo \"Files moved to production.\"\n",
    "        else\n",
    "            echo \"No updates detected in the changelog.\"\n",
    "        fi\n",
    "    fi\n",
    "else\n",
    "    echo \"Expected files not generated. Check the main script.\"\n",
    "fi\n",
    "```\n",
    "\n",
    "### Script Explanation\n",
    "\n",
    "1. **Directories**: Defines the paths for the development and production directories, as well as the log and changelog files.\n",
    "2. **Create Production Directory**: Checks if the production directory exists and creates it if it doesn't.\n",
    "3. **Run Main Script**: Uses `python` to run your main script (`data_pipeline.py`).\n",
    "4. **Check Generated Files**: Verifies that the files `cademycode_clean_local.db` and `cademycode_final_local.csv` have been created in the development directory.\n",
    "5. **Run Unit Tests**: Uses `python -m unittest` to run the unit tests (`test_data_pipeline.py`).\n",
    "6. **Check Logs**: Uses `grep` to search for errors in the log file.\n",
    "7. **Check for Updates**: Uses `grep` to search for the word \"Version\" in the changelog. If found, it assumes there was an update.\n",
    "8. **Move Files**: If an update is detected, uses `mv` to move the files to the production directory.\n",
    "\n",
    "### Make the Script Executable\n",
    "\n",
    "Ensure your Bash script is executable. You can do this with the following command:\n",
    "\n",
    "```bash\n",
    "chmod +x /path/to/your_script.sh -> chmod +x run_pipeline.sh\n",
    "```\n",
    "\n",
    "### Run the Script\n",
    "\n",
    "Finally, run your Bash script:\n",
    "\n",
    "```bash\n",
    "/path/to/your_script.sh -> ./run_pipeline.sh / bash run_pipeline.sh\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Create the Bash Script**: Define directories, run the Python script, check generated files, run unit tests, check logs, and verify the changelog.\n",
    "- **Create Production Directory**: Create the production directory if it doesn't exist.\n",
    "- **Move Files**: If an update is detected, move the files to the production directory.\n",
    "- **Make the Script Executable**: Use `chmod +x` to make the script executable.\n",
    "- **Run the Script**: Execute the Bash script to complete the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ bash run_pipeline.sh\n",
    "Directorio de producción ya existe: ./prod\n",
    "Ejecutando el script principal de Python.\n",
    "Length of DataFrame before merge: 6000\n",
    "Length of DataFrame after merge: 8301\n",
    "Missing values in the final DataFrame: uuid                        0\n",
    "name                        0\n",
    "dob                         0\n",
    "sex                         0\n",
    "contact_info                0\n",
    "job_id                      0\n",
    "num_course_taken          371\n",
    "current_career_path_id      0\n",
    "time_spent_hrs            676\n",
    "job_category                0\n",
    "avg_salary                  0\n",
    "career_path_id            676\n",
    "career_path_name          676\n",
    "hours_to_complete         676\n",
    "dtype: int64\n",
    "Final CSV and SQLite database created successfully.\n",
    "Archivos generados correctamente.\n",
    "Ejecutando las pruebas unitarias.\n",
    "...\n",
    "----------------------------------------------------------------------\n",
    "Ran 3 tests in 0.080s\n",
    "\n",
    "OK\n",
    "El n▒mero de filas se ha mantenido igual.\n",
    "Revisando los registros para detectar errores.\n",
    "No se encontraron errores en el log.\n",
    "Verificando si hubo una actualización en el changelog.\n",
    "Actualización detectada. Moviendo archivos a producción...\n",
    "Moviendo cademycode_clean_local.db a producción.\n",
    "Archivo cademycode_clean_local.db movido a producción.\n",
    "Moviendo cademycode_final_local.csv a producción.\n",
    "Archivo cademycode_final_local.csv movido a producción.\n",
    "Archivos movidos a producción.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Readme\n",
    "Create a readme.md file for your update process. This file should describe the folder structure of your final project and include instructions for how to run your Bash script to update the database.\n",
    "\n",
    "****\n",
    "\n",
    "Think about what information someone would need to know to run your update process:\n",
    "\n",
    "+ what each file does, and where it lives\n",
    "+ how to run the update process\n",
    "+ how any changelogs or other version control systems you’ve created operate\n",
    "+ where errors are logged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the updated `README.md` file translated into English with additional details about the changelog, version control system, and error logs.\n",
    "\n",
    "### README.md\n",
    "\n",
    "```markdown\n",
    "# Data Pipeline Update Process\n",
    "\n",
    "This project contains a data pipeline that processes information from an SQLite database, generates a CSV file and a new SQLite database, and moves the generated files to a production directory. Below is a description of the project folder structure, instructions for running the Bash script to update the database, and how the changelogs and error logs work.\n",
    "\n",
    "## Folder Structure\n",
    "\n",
    "```\n",
    "project-root/\n",
    "│\n",
    "├── dev/\n",
    "│   ├── cademycode_clean_local.db\n",
    "│   ├── cademycode_final_local.csv\n",
    "│   ├── changelog.txt\n",
    "│   ├── data_pipeline.log\n",
    "│   └── previous_row_count.txt\n",
    "│\n",
    "├── prod/\n",
    "│   ├── cademycode_clean_local.db\n",
    "│   ├── cademycode_final_local.csv\n",
    "│\n",
    "├── data_pipeline.py\n",
    "├── test_data_pipeline.py\n",
    "└── run_pipeline.sh\n",
    "```\n",
    "\n",
    "- **dev/**: Development directory where temporary files and logs are generated and stored.\n",
    "  - `cademycode_clean_local.db`: SQLite database generated after processing the data.\n",
    "  - `cademycode_final_local.csv`: CSV file generated after processing the data.\n",
    "  - `changelog.txt`: Changelog file that records the details of each update.\n",
    "  - `data_pipeline.log`: Log file that records the details of the script execution.\n",
    "  - `previous_row_count.txt`: File that stores the row count of the previous version of the table.\n",
    "\n",
    "- **prod/**: Production directory where the generated files are moved after a successful update.\n",
    "  - `cademycode_clean_local.db`: SQLite database generated after processing the data.\n",
    "  - `cademycode_final_local.csv`: CSV file generated after processing the data.\n",
    "\n",
    "- **data_pipeline.py**: Main Python script that processes the data and generates the files.\n",
    "- **test_data_pipeline.py**: Unit test script to verify that the data pipeline works correctly.\n",
    "- **run_pipeline.sh**: Bash script that runs the main Python script, verifies the generated files, runs the unit tests, checks the logs, and moves the files to production if an update is detected.\n",
    "\n",
    "## Instructions for Running the Update Process\n",
    "\n",
    "1. **Activate the Virtual Environment**: Make sure to activate your virtual environment before running the Bash script.\n",
    "\n",
    "```bash\n",
    "source \"C:/Users/gabri_7a484pu/Escritorio/code/DataEngineer_Codecademy/.venv/Scripts/activate\"\n",
    "```\n",
    "\n",
    "2. **Run the Bash Script**: Run the Bash script to process the data and move the generated files to production.\n",
    "\n",
    "```bash\n",
    "bash run_pipeline.sh\n",
    "```\n",
    "\n",
    "## Changelog and Version Control System\n",
    "\n",
    "- **Changelog**: The `dev/changelog.txt` file records the details of each update, including the number of new rows added and the amount of missing data. The Bash script checks this file to detect updates and move the files to production if a new version is detected. Example changelog content:\n",
    "\n",
    "```plaintext\n",
    "## [2025-02-26 16:23:09]\n",
    "### Added\n",
    "- New rows added: 2006\n",
    "- Missing data counts: 2981\n",
    "### Fixed\n",
    "- Fixed missing table errors for `cademycode_courses`\n",
    "### Changed\n",
    "- Combined tables into `cademycode_final_local`\n",
    "\n",
    "Version 1.0.1\n",
    "- New rows added: 100\n",
    "- Missing data counts: 0\n",
    "\n",
    "## [2025-02-26 16:25:51]\n",
    "### Added\n",
    "- New rows added: 2006\n",
    "- Missing data counts: 2981\n",
    "### Fixed\n",
    "- Fixed missing table errors for `cademycode_courses`\n",
    "### Changed\n",
    "- Combined tables into `cademycode_final_local`\n",
    "\n",
    "## [2025-02-26 16:28:16]\n",
    "### Added\n",
    "- New rows added: 2301\n",
    "- Missing data counts: 3075\n",
    "### Fixed\n",
    "- Fixed missing table errors for `cademycode_courses`\n",
    "### Changed\n",
    "- Combined tables into `cademycode_final_local`\n",
    "\n",
    "## [2025-02-26 16:28:50]\n",
    "### Added\n",
    "- New rows added: 2301\n",
    "- Missing data counts: 3075\n",
    "### Fixed\n",
    "- Fixed missing table errors for `cademycode_courses`\n",
    "### Changed\n",
    "- Combined tables into `cademycode_final_local`\n",
    "\n",
    "## [2025-02-26 16:31:47]\n",
    "### Added\n",
    "- New rows added: 2301\n",
    "- Missing data counts: 3075\n",
    "### Fixed\n",
    "- Fixed missing table errors for `cademycode_courses`\n",
    "### Changed\n",
    "- Combined tables into `cademycode_final_local`\n",
    "```\n",
    "\n",
    "- **Error Logs**: The `dev/data_pipeline.log` file records the details of the script execution, including any errors that occur during the process. You can review this file for more information about any issues that arise. Example log content:\n",
    "\n",
    "```plaintext\n",
    "2025-02-26 16:23:09,279:INFO:Reading data from the database.\n",
    "2025-02-26 16:23:09,281:INFO:Data from cademycode_courses read successfully.\n",
    "2025-02-26 16:23:09,282:INFO:Data from cademycode_student_jobs read successfully.\n",
    "2025-02-26 16:23:09,300:INFO:Data from cademycode_students read successfully.\n",
    "2025-02-26 16:23:09,300:INFO:Converting job_id columns to integers.\n",
    "2025-02-26 16:23:09,304:INFO:Merging tables.\n",
    "2025-02-26 16:23:09,313:INFO:Length of DataFrame before merge: 5000\n",
    "2025-02-26 16:23:09,314:INFO:Length of DataFrame after merge: 7006\n",
    "2025-02-26 16:23:09,317:INFO:Missing values in the final DataFrame: uuid                        0\n",
    "name                        0\n",
    "dob                         0\n",
    "sex                         0\n",
    "contact_info                0\n",
    "job_id                      0\n",
    "num_course_taken          353\n",
    "current_career_path_id      0\n",
    "time_spent_hrs            657\n",
    "job_category                0\n",
    "avg_salary                  0\n",
    "career_path_id            657\n",
    "career_path_name          657\n",
    "hours_to_complete         657\n",
    "dtype: int64\n",
    "2025-02-26 16:23:09,319:INFO:Saving the final DataFrame to a new SQLite database.\n",
    "2025-02-26 16:23:09,416:INFO:Saving the final DataFrame to a CSV file.\n",
    "2025-02-26 16:23:09,475:INFO:Final CSV and SQLite database created successfully.\n",
    "```\n",
    "\n",
    "- **Previous Row Count**: The `dev/previous_row_count.txt` file stores the row count of the previous version of the table for comparison with the current count. Example file content:\n",
    "\n",
    "```plaintext\n",
    "8301\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Writeup\n",
    "Write a high-level overview of the project to share in your portfolio. This is for your portfolio, so include your thought process in addition to the actual steps you took.\n",
    "\n",
    "****\n",
    "\n",
    "Here are some prompts to help you get started:\n",
    "\n",
    "+ What data cleaning operations did you do, and why?\n",
    "+ What kinds of unit tests did you use? Why did you pick those?\n",
    "+ How did you structure the automation of the bash script?\n",
    "+ What steps did you take to protect the final database from being incorrectly updated?\n",
    "\n",
    "Remember, you are trying to demonstrate your skills in:\n",
    "\n",
    "+ Data cleaning and wrangling\n",
    "+ Unit tests and error logging\n",
    "+ Bash scripting\n",
    "+ SQLite databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Writeup: Data Pipeline Automation\n",
    "\n",
    "#### High-Level Overview\n",
    "\n",
    "In this project, I developed a data pipeline to process information from a SQLite database, generate a CSV file and a new SQLite database, and automate the movement of these files to a production directory. The project involved data cleaning, unit testing, error logging, bash scripting, and working with SQLite databases. Below, I detail the steps taken and the thought process behind each task.\n",
    "\n",
    "----\n",
    "\n",
    "#### Data Cleaning and Wrangling Operations\n",
    "\n",
    "**What data cleaning and wrangling operations did you do, and why?**\n",
    "\n",
    "To ensure the integrity and usability of the data, I performed several data cleaning and wrangling operations:\n",
    "\n",
    "1. **Reading Data**: I read data from three tables (`cademycode_courses`, `cademycode_student_jobs`, and `cademycode_students`) from the SQLite database.\n",
    "2. **Handling Non-Integer Values**: I converted the `job_id` columns in the `df_students` and `df_jobs` DataFrames to integers, handling any non-integer values by coercing them to `NaN` and then filling them with `0`. This ensured consistency in data types, which is crucial for accurate merging and analysis.\n",
    "3. **Merging Tables**: I merged the `df_students` DataFrame with the `df_jobs` DataFrame on the `job_id` column, and then merged the result with the `df_courses` DataFrame on the `current_career_path_id` and `career_path_id` columns. This step combined all relevant information into a single DataFrame for easier analysis and reporting.\n",
    "4. **Checking for Missing Values**: After merging, I checked for any missing values in the final DataFrame and logged the results. Identifying missing values helps in understanding data quality and planning further cleaning steps if necessary.\n",
    "\n",
    "These operations were essential to ensure that the data was clean, consistent, and ready for analysis and reporting.\n",
    "\n",
    "----\n",
    "\n",
    "#### Unit Tests\n",
    "\n",
    "**What kinds of unit tests did you use? Why did you pick those?**\n",
    "\n",
    "I implemented the following unit tests to ensure the correctness and reliability of the data pipeline:\n",
    "\n",
    "1. **Schema Test**: This test checks that the updated database contains the expected final table (`cademycode_final_local`). It ensures that the schema of the database is as expected after the data processing.\n",
    "2. **New Data Test**: This test verifies that new data has been added to the final table by comparing the current row count with the previous row count. It ensures that the pipeline is correctly adding new data and not just overwriting existing data.\n",
    "3. **Table Joins Test**: Although this test was initially included to check if tables join properly, it was later deemed unnecessary as the final database only contains the combined table. However, it highlights the importance of verifying data integrity during the merging process.\n",
    "\n",
    "These tests were chosen to validate the critical aspects of the data pipeline, ensuring that the data processing steps produce the expected results and that new data is correctly integrated.\n",
    "\n",
    "----\n",
    "\n",
    "#### Bash Script Automation\n",
    "\n",
    "**How did you structure the automation of the bash script?**\n",
    "\n",
    "The bash script (`run_pipeline.sh`) was structured to automate the entire data pipeline process, including:\n",
    "\n",
    "1. **Directory Setup**: The script checks if the production directory (`prod`) exists and creates it if it doesn't. This ensures that the environment is correctly set up before processing begins.\n",
    "2. **Activating the Virtual Environment**: The script activates the Python virtual environment to ensure that all necessary dependencies are available.\n",
    "3. **Executing the Python Script**: The script runs the main Python script (`data_pipeline.py`) to process the data and generate the output files.\n",
    "4. **Running Unit Tests**: After generating the files, the script runs the unit tests (`test_data_pipeline.py`) to verify the correctness of the data processing.\n",
    "5. **Checking Logs for Errors**: The script checks the log file (`data_pipeline.log`) for any errors that occurred during the execution of the Python script.\n",
    "6. **Verifying Changelog Updates**: The script checks the changelog (`changelog.txt`) for updates. If an update is detected, it moves the generated files to the production directory.\n",
    "\n",
    "This structure ensures that the entire process is automated, from data processing to verification and deployment, minimizing the risk of human error.\n",
    "\n",
    "----\n",
    "\n",
    "#### Protecting the Final Database\n",
    "\n",
    "**What steps did you take to protect the final database from being incorrectly updated?**\n",
    "\n",
    "To protect the final database from being incorrectly updated, I implemented the following steps:\n",
    "\n",
    "1. **Unit Tests**: The unit tests ensure that the data processing steps produce the expected results and that new data is correctly integrated. This helps catch any issues before the final database is updated.\n",
    "2. **Error Logging**: The script logs detailed information about each step of the process, including any errors that occur. This allows for quick identification and resolution of issues, preventing incorrect updates.\n",
    "3. **Changelog Verification**: The script checks the changelog for updates before moving the generated files to the production directory. This ensures that only valid updates are deployed.\n",
    "4. **Row Count Comparison**: The script compares the current row count with the previous row count to ensure that new data has been added. This prevents overwriting existing data without adding new information.\n",
    "\n",
    "These steps help ensure the integrity and reliability of the final database, protecting it from incorrect updates.\n",
    "\n",
    "----\n",
    "\n",
    "#### Demonstrated Skills\n",
    "\n",
    "Throughout this project, I demonstrated the following skills:\n",
    "\n",
    "- **Data Cleaning and Wrangling**: I performed various data cleaning operations to ensure the integrity and usability of the data.\n",
    "- **Unit Tests and Error Logging**: I implemented unit tests to validate the data processing steps and used error logging to track and resolve issues.\n",
    "- **Bash Scripting**: I structured and automated the data pipeline process using a bash script, ensuring a seamless and error-free workflow.\n",
    "- **SQLite Databases**: I worked extensively with SQLite databases, reading data, processing it, and generating new databases for reporting and analysis.\n",
    "\n",
    "This project showcases my ability to design and implement a robust data pipeline, ensuring data quality, reliability, and automation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Your Project\n",
    "\n",
    "If you’re taking the Data Engineering Career Path, you’ll use this project later on. Make sure to save your project somewhere you can find again! Double-check that your code is commented and your readme and writeup are thorough."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
